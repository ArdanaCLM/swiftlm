# (c) Copyright 2015,2016 Hewlett Packard Enterprise Development LP
# (c) Copyright 2017 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#

import json
import os
from shutil import rmtree
import socket
import subprocess
import tempfile
import unittest
import fcntl
import errno
import mock
import time

from swiftlm.swift import file_ownership, swift_services
from swiftlm.utils import utility
from swiftlm.utils.values import ServerType
from tests import (create_valid_proxy_fileset, create_valid_non_proxy_fileset,
                   create_fake_process_entries, FakeLogger)
from tests.data import drive_audit_data

# TODO add tests for entry points 'drive-info'

TEST_MODULE = 'swiftlm.monasca.check_plugins.swiftlm_check'


# This is a horrible hack to work around monasca_agent.common.config.Config
# expecting to find a config file in the default location(s) when it is first
# constructed as a result of importing the swift ln check's superclass (
# monasca_agent.collector.checks.check.AgentCheck).
#
# Since Config is a singleton we create a temp config file, construct Config
# singleton and then move on, but this must be done before other imports from
# monasca.
#
# Doing this is a setup_module or setup_package method did not work.
def _create_agent_conf():
    # create a temp conf file
    tempdir = tempfile.mkdtemp()
    conf_file = os.path.join(tempdir, 'agent.yaml')
    with open(conf_file, 'wb') as fd:
        fd.write(
            """
            Logging:
              collector_log_file: /var/log/monasca/agent/collector.log
              forwarder_log_file: /var/log/monasca/agent/forwarder.log
              log_level: DEBUG
              statsd_log_file: /var/log/monasca/agent/statsd.log
            Main:
              check_freq: 60
              dimensions: {}
              hostname: example.com
            """
        )
    # construct singleton Config instance
    import monasca_agent.common.config

    config = monasca_agent.common.config.Config(conf_file)
    # clean up
    rmtree(tempdir, ignore_errors=True)
    return config

config = _create_agent_conf()

# import from monasca after running _create_agent_conf()...
from monasca_agent.collector.checks.collector import Collector  # noqa
from swiftlm.monasca.check_plugins import swiftlm_check  # noqa


def _dump_measurements(measurements):
    lines = []
    for m in measurements:
        lines.append(
            'name: %s, value: %s, %s %s %s'
            % (m.name, m.value, m.dimensions, m.timestamp, m.value_meta))
    return '\n'.join(lines)


class BaseTest(unittest.TestCase):
    # each subclass should set task_name to the value expected in the
    # expected measurements' name attribute
    task_name = None
    # Each subclass should set the lists of entry points and/or commands
    # to be patched into plugin for their tests.
    task_entry_points = None
    sub_commands = None

    def setUp(self):
        agent_conf = config.get_config('Main')
        # setup an instance of the check plugin for tests
        init_conf = None
        instance_dimensions = {}
        instances = [{'dimensions': instance_dimensions}]
        fake_logger = FakeLogger()
        self.check = swiftlm_check.SwiftLMScan(
            'swiftlm-scan', init_conf, agent_conf, instances,
            logger=fake_logger)

        # Override the default list of task entry points that the check plugin
        # loads so that we can restrict testing to subsets of entry points.
        # (This reduces the amount of system config faking required to run
        # individual tests).
        if self.task_entry_points is not None:
            self.check.TASK_ENTRY_POINTS = self.task_entry_points

        # Override the default list of sub-commands that the check runs
        if self.sub_commands is not None:
            self.check.DEFAULT_SUBCOMMANDS = self.sub_commands

        # setup an instance of the monasca collector which will
        # be used to call the check plugin
        checks = {'initialized_checks': [self.check],
                  'init_failed_checks': {}}
        self.collector = Collector({}, None, checksd=checks)

        # setup a temp dir for tests
        self.testdir = tempfile.mkdtemp()
        self.fake_time = int(time.time())

        # setup base expected values
        # base_dimensions are generated by swiftlm MetricData
        self.expected_dimensions_base = {'service': 'object-storage'}
        # agent conf takes precedence over check dimensions
        self.expected_dimensions_base.update(agent_conf.get('dimensions', {}))
        # plugin instance conf takes precedence over agent dimensions
        self.expected_dimensions_base.update(instance_dimensions)
        # most tests report metrics with hostname set in dimensions...
        self.expected_dimensions_base['hostname'] = socket.gethostname()

        self.expected_measurement_base = dict(name=self.task_name)

    def tearDown(self):
        rmtree(self.testdir, ignore_errors=True)

    def _is_equivalent(self, expected, metric):
        for key in expected:
            if expected[key] != getattr(metric, key):
                return False
        return True

    def _assert_expected_measurements(self, expected, actual):
        """
        Verify that a list of expected metrics dicts is equivalent
        to a list of actual Measurement instances.
        """
        expected = list(expected)
        self.assertEqual(len(expected), len(actual))
        # shenanigans to deal with results order being unpredictable
        for measurement in list(actual):
            for metric in list(expected):
                if self._is_equivalent(metric, measurement):
                    actual.remove(measurement)
                    expected.remove(metric)
                    break
            else:
                self.fail('Unexpected measurement:\n %s\nnot found in:\n%s'
                          % (_dump_measurements([measurement]), expected))
        self.assertFalse(expected, 'Expected values left over %s' % expected)


def _make_fake_load_instance_conf(metrics_files=None, subcommands=None,
                                  suppress_ok=None):
    def _fake_load_instance_conf(self, instance):
        self.metrics_files = metrics_files or []
        self.subcommands = subcommands or []
        self.suppress_ok = suppress_ok or []
    return _fake_load_instance_conf


class TestConfig(BaseTest):
    def test_no_instance_config(self):
        conf = {}
        # don't bother with tasks, we are just verifying conf processing
        with mock.patch(TEST_MODULE + '.SwiftLMScan._get_metrics') as mocked:
            mocked.return_value = [], {}
            self.check.check(conf)
        self.assertEqual(swiftlm_check.SwiftLMScan.DEFAULT_SUPPRESS_OK,
                         self.check.suppress_ok)
        self.assertEqual(swiftlm_check.SwiftLMScan.DEFAULT_SUBCOMMANDS,
                         self.check.subcommands)
        self.assertEqual([], self.check.metrics_files)

    def test_load_instance_config(self):
        conf = {'metrics_files': 'file1,file2',
                'subcommands': 'drive-audit,connectivity,check_mounts',
                'suppress_ok': 'drive-audit,connectivity'}
        # don't bother with tasks, we are just verifying conf processing
        with mock.patch(TEST_MODULE + '.SwiftLMScan._get_metrics') as mocked:
            mocked.return_value = [], {}
            self.check.check(conf)
        self.assertEqual(['drive-audit', 'connectivity'],
                         self.check.suppress_ok)
        self.assertEqual(['drive-audit', 'connectivity', 'check_mounts'],
                         self.check.subcommands)
        self.assertEqual(['file1', 'file2'], self.check.metrics_files)


class TestErrors(BaseTest):
    task_name = swiftlm_check.MODULE_METRIC_NAME
    sub_commands = []

    def test_missing_entry_point_reported(self):
        self.check.TASK_ENTRY_POINTS = ['does_not_exist']
        expected_dimensions = dict(self.expected_dimensions_base)
        expected_dimensions.update(dict(task='does_not_exist'))
        expected_value_meta = dict(
            msg='Entry point not found for task: "does_not_exist"')
        expected_metric = dict(self.expected_measurement_base)
        expected_metric.update(dict(dimensions=expected_dimensions,
                                    value=2,
                                    value_meta=expected_value_meta))
        expected = [expected_metric]
        with mock.patch('swiftlm.utils.metricdata.timestamp',
                        lambda *args: self.fake_time):
            actual, events, statuses = self.collector.run_checks_d()

        self._assert_expected_measurements(expected, actual)
        log_lines = self.check.log.get_lines_for_level('info')
        self.assertEqual(1, len(log_lines),
                         'Found log lines %s' % log_lines)
        self.assertTrue(log_lines[0].startswith('Ran 1 entry point task.'),
                        log_lines)
        self.assertFalse(self.check.log.errors_logged)

        self.assertFalse(self.check.log.errors_logged)

    def test_task_exception_reported(self):
        self.check.TASK_ENTRY_POINTS = ['file-ownership']
        expected_dimensions = dict(self.expected_dimensions_base)
        expected_dimensions.update(dict(type='plugin',
                                        task='file-ownership'))
        expected_value_meta = dict(
            msg='Plugin task execution failed: "file-ownership"')
        expected_metric = dict(self.expected_measurement_base)
        expected_metric.update(dict(dimensions=expected_dimensions,
                                    value=swiftlm_check.FAIL,
                                    value_meta=expected_value_meta))
        expected = [expected_metric]
        with mock.patch('swiftlm.utils.metricdata.timestamp',
                        lambda *args: self.fake_time):
            with mock.patch('swiftlm.swift.file_ownership.main') as mocked:
                mocked.side_effect = Exception('kaboooom!')
                actual, events, statuses = self.collector.run_checks_d()

        self._assert_expected_measurements(expected, actual)
        log_lines = self.check.log.get_lines_for_level('info')
        self.assertEqual(1, len(log_lines),
                         'Found log lines %s' % log_lines)
        self.assertTrue(log_lines[0].startswith('Ran 1 entry point task.'),
                        log_lines)
        log_lines = self.check.log.get_lines_for_level('warn')
        self.assertEqual(1, len(log_lines),
                         'Found log lines %s' % log_lines)
        self.assertEqual(
            'Plugin task "file-ownership" failed with "kaboooom!"',
            log_lines[0], log_lines)

    def test_agent_exception_logged(self):
        self.check.TASK_ENTRY_POINTS = ['does-not-exist']
        with mock.patch(TEST_MODULE + '.SwiftLMScan.gauge') as mocked:
            mocked.side_effect = Exception('kaboooom!')
            actual, events, statuses = self.collector.run_checks_d()

        self.assertFalse(actual)
        log_lines = self.check.log.get_lines_for_level('exception')
        self.assertEqual(1, len(log_lines),
                         'Found log lines %s' % log_lines)
        self.assertTrue(log_lines[0].startswith('Exception while reporting'),
                        log_lines)


class TestAllTasks(BaseTest):
    sub_commands = []

    def test_all_tasks(self):
        # Sanity check to ensure that nothing blows up. It should be safe
        # to run the agent check on a host that does not have swift installed,
        # but we can't make assertions about metrics because we don't know if
        # swift is installed on the test host.
        self.check.TASK_ENTRY_POINTS = self.check.TASKS
        actual, events, statuses = self.collector.run_checks_d()
        log_lines = self.check.log.get_lines_for_level('info')
        self.assertEqual(1, len(log_lines),
                         'All log lines %s' % self.check.log.dump())
        self.assertTrue(
            log_lines[0].startswith('Ran %d entry point tasks.'
                                    % len(self.check.TASK_ENTRY_POINTS)),
            'Unexpected log lines: %s' % str(log_lines))

        log_lines = self.check.log.get_lines_for_level('warn')
        self.assertEqual(0, len(log_lines),
                         'Unexpected warns %s' % self.check.log.dump())
        self.assertFalse(
            self.check.log.errors_logged,
            'Unexpected error logs: %s'
            % self.check.log.dump())


class TestFileOwnership(BaseTest):
    task_name = 'swiftlm.swift.file_ownership'
    task_entry_points = ['file-ownership']
    sub_commands = []

    def setUp(self):
        super(TestFileOwnership, self).setUp()
        # setup fake directories and patch file_ownership to use them
        self.etc_dir = os.path.join(self.testdir, 'etc')
        os.makedirs(self.etc_dir)
        self.srv_dir = os.path.join(self.testdir, 'srv')
        os.makedirs(self.srv_dir)
        self.orig_conf_dir = file_ownership.CONF_DIR
        self.orig_swift_dir = file_ownership.SWIFT_DIR
        self.orig_node_dir = file_ownership.NODE_DIR
        file_ownership.SWIFT_DIR = os.path.join(self.etc_dir, 'swift')
        file_ownership.CONF_DIR = self.etc_dir
        file_ownership.NODE_DIR = os.path.join(self.srv_dir, 'node')

    def tearDown(self):
        file_ownership.SWIFT_DIR = self.orig_swift_dir
        file_ownership.CONF_DIR = self.orig_conf_dir
        file_ownership.NODE_DIR = self.orig_node_dir
        super(TestFileOwnership, self).tearDown()

    def test_missing_files_on_non_proxy(self):
        expected = []
        paths = [os.path.join(self.srv_dir, 'node'),
                 os.path.join(self.etc_dir, 'rsyncd.conf'),
                 os.path.join(self.etc_dir, 'rsyslog.conf'),
                 os.path.join(self.etc_dir, 'swift')]
        for path in paths:
            expected_dimensions = dict(self.expected_dimensions_base)
            expected_dimensions.update({'path': path})
            expected_value_meta = {'msg': 'Path: %s is missing' % path}
            expected_measurement = dict(self.expected_measurement_base)
            expected_measurement.update(dict(dimensions=expected_dimensions,
                                             value=swiftlm_check.FAIL,
                                             value_meta=expected_value_meta))
            expected.append(expected_measurement)
        with mock.patch('swiftlm.swift.file_ownership.server_type',
                        lambda x: x != ServerType.proxy):
            with mock.patch('swiftlm.utils.metricdata.timestamp',
                            lambda *args: self.fake_time):
                actual, events, statuses = self.collector.run_checks_d()
        self._assert_expected_measurements(expected, actual)
        # TODO verify events, statuses

    def test_missing_files_on_proxy(self):
        # proxy server not required to have rsyncd.conf or /srv/node
        expected = []
        paths = [os.path.join(self.etc_dir, 'rsyslog.conf'),
                 os.path.join(self.etc_dir, 'swift')]
        for path in paths:
            expected_dimensions = dict(self.expected_dimensions_base)
            expected_dimensions.update({'path': path})
            expected_value_meta = {'msg': 'Path: %s is missing' % path}
            expected_measurement = dict(self.expected_measurement_base)
            expected_measurement.update(dict(dimensions=expected_dimensions,
                                             value=swiftlm_check.FAIL,
                                             value_meta=expected_value_meta))
            expected.append(expected_measurement)
        with mock.patch('swiftlm.swift.file_ownership.server_type',
                        lambda x: x == ServerType.proxy):
            with mock.patch('swiftlm.utils.metricdata.timestamp',
                            lambda *args: self.fake_time):
                actual, events, statuses = self.collector.run_checks_d()
        self._assert_expected_measurements(expected, actual)

    def test_all_present_and_correct_non_proxy(self):
        # no metrics are reported when all is ok
        create_valid_non_proxy_fileset(self.etc_dir, self.srv_dir)
        with mock.patch('swiftlm.swift.file_ownership.server_type',
                        lambda x: x != ServerType.proxy):
            with mock.patch('swiftlm.utils.metricdata.timestamp',
                            lambda *args: self.fake_time):
                with mock.patch('pwd.getpwuid') as mock_pwuid:
                    mock_pwuid.return_value = mock.Mock(pw_name='swift')
                    actual, events, statuses = self.collector.run_checks_d()
        self.assertFalse(actual)

    def test_all_present_and_correct_proxy(self):
        # no metrics are reported when all is ok
        create_valid_proxy_fileset(self.etc_dir)
        with mock.patch('swiftlm.swift.file_ownership.server_type',
                        lambda x: x == ServerType.proxy):
            with mock.patch('swiftlm.utils.metricdata.timestamp',
                            lambda *args: self.fake_time):
                with mock.patch('pwd.getpwuid') as mock_pwuid:
                    mock_pwuid.return_value = mock.Mock(pw_name='swift')
                    actual, events, statuses = self.collector.run_checks_d()
        self.assertFalse(actual,
                         'Unexpected measurements found %s'
                         % _dump_measurements(actual))


class TestSwiftServices(BaseTest):
    task_name = 'swiftlm.swift.swift_services'
    task_entry_points = ['swift-services']
    sub_commands = []

    def setUp(self):
        super(TestSwiftServices, self).setUp()
        # setup fake proc directory
        self.proc_dir = os.path.join(self.testdir, 'proc')
        os.makedirs(self.proc_dir)

    def tearDown(self):
        super(TestSwiftServices, self).tearDown()

    def test_all_services_running(self):
        server_types = dict(proxy=True,
                            object=True,
                            container=True,
                            account=True)
        expected_services = list(s for s in swift_services.SERVICES
                                 if s.startswith(tuple(server_types.keys())))
        create_fake_process_entries(self.proc_dir, expected_services)
        expected = []
        for service in expected_services:
            expected_dimensions = dict(self.expected_dimensions_base)
            expected_dimensions.update(dict(component=service))
            expected_value_meta = dict(msg=service + ' is running')
            expected_metric = dict(self.expected_measurement_base)
            expected_metric.update(dict(dimensions=expected_dimensions,
                                        value=swiftlm_check.OK,
                                        value_meta=expected_value_meta))
            expected.append(expected_metric)

        with mock.patch('swiftlm.utils.metricdata.timestamp',
                        lambda *args: self.fake_time):
            with mock.patch('swiftlm.swift.swift_services.server_type',
                            lambda *args: server_types):
                with mock.patch('swiftlm.swift.swift_services.PROC_DIR',
                                self.proc_dir):
                    # force ok metrics to be reported
                    self.check.DEFAULT_SUPPRESS_OK = []
                    actual, events, statuses = self.collector.run_checks_d()
        self._assert_expected_measurements(expected, actual)

    def test_no_services_running(self):
        server_types = dict(proxy=True,
                            object=True,
                            container=True,
                            account=True)
        expected_services = list(s for s in swift_services.SERVICES
                                 if s.startswith(tuple(server_types.keys())))
        expected = []
        for service in expected_services:
            expected_dimensions = dict(self.expected_dimensions_base)
            expected_dimensions.update(dict(component=service))
            expected_value_meta = dict(msg=service + ' is not running')
            expected_metric = dict(self.expected_measurement_base)
            expected_metric.update(dict(dimensions=expected_dimensions,
                                        value=swiftlm_check.FAIL,
                                        value_meta=expected_value_meta))
            expected.append(expected_metric)

        with mock.patch('swiftlm.utils.metricdata.timestamp',
                        lambda *args: self.fake_time):
            with mock.patch('swiftlm.swift.swift_services.server_type',
                            lambda *args: server_types):
                with mock.patch('swiftlm.swift.swift_services.PROC_DIR',
                                self.proc_dir):
                    actual, events, statuses = self.collector.run_checks_d()
        self._assert_expected_measurements(expected, actual)

    def test_no_server_conf_found(self):
        server_types = dict(proxy=False,
                            object=False,
                            container=False,
                            account=False)
        expected_dimensions = dict(self.expected_dimensions_base)
        expected_value_meta = dict(msg='no swift services running')
        expected_metric = dict(self.expected_measurement_base)
        expected_metric.update(dict(dimensions=expected_dimensions,
                                    value=swiftlm_check.UNKNOWN,
                                    value_meta=expected_value_meta))
        expected = [expected_metric]
        with mock.patch('swiftlm.utils.metricdata.timestamp',
                        lambda *args: self.fake_time):
            with mock.patch('swiftlm.swift.swift_services.server_type',
                            lambda *args: server_types):
                with mock.patch('swiftlm.swift.swift_services.PROC_DIR',
                                self.proc_dir):
                    actual, events, statuses = self.collector.run_checks_d()
        self._assert_expected_measurements(expected, actual)


class TestDriveAudit(BaseTest):
    task_name = 'swiftlm.swift.drive_audit'
    task_entry_points = ['drive-audit']
    sub_commands = []

    def test_handles_OS_error(self):
        with mock.patch('subprocess.Popen') as mocked:
            mocked.side_effect = OSError('blah')
            with mock.patch('swiftlm.utils.metricdata.timestamp',
                            lambda *args: self.fake_time):
                actual, events, statuses = self.collector.run_checks_d()
        expected_dimensions = dict(self.expected_dimensions_base)
        expected_dimensions.update(dict(error='blah'))
        expected_value_meta = dict(msg='Unrecoverable error: blah')
        expected_metric = dict(self.expected_measurement_base)
        expected_metric.update(dict(dimensions=expected_dimensions,
                                    value=swiftlm_check.UNKNOWN,
                                    value_meta=expected_value_meta))
        self._assert_expected_measurements([expected_metric], actual)

    def test_drives_found_and_no_errors(self):
        with mock.patch('subprocess.Popen') as mocked:
            mocked.return_value = mock.MagicMock(
                spec=subprocess.Popen,
                communicate=lambda: (None, drive_audit_data.output1))
            with mock.patch('swiftlm.utils.metricdata.timestamp',
                            lambda *args: self.fake_time):
                self.check.DEFAULT_SUPPRESS_OK = []
                actual, events, statuses = self.collector.run_checks_d()

        scenarios = (('sdc', swiftlm_check.OK,
                      'No errors found on device: sdc'),
                     ('sdd', swiftlm_check.OK,
                      'No errors found on device: sdd'))
        expected = []
        for scenario in scenarios:
            expected_dimensions = dict(self.expected_dimensions_base)
            expected_dimensions.update({'device': scenario[0]})
            expected_value_meta = dict(msg=scenario[2])
            expected_metric = dict(self.expected_measurement_base)
            expected_metric.update(dict(dimensions=expected_dimensions,
                                        value=scenario[1],
                                        value_meta=expected_value_meta))
            expected.append(expected_metric)
        self._assert_expected_measurements(expected, actual)

    def test_drives_found_and_errors(self):
        with mock.patch('subprocess.Popen') as mocked:
            mocked.return_value = mock.MagicMock(
                spec=subprocess.Popen,
                communicate=lambda: (None, drive_audit_data.output3))
            with mock.patch('swiftlm.utils.metricdata.timestamp',
                            lambda *args: self.fake_time):
                self.check.DEFAULT_SUPPRESS_OK = []
                actual, events, statuses = self.collector.run_checks_d()

        expected = []
        scenarios = (('sdb', swiftlm_check.FAIL,
                      'Errors found on device: sdb'),
                     ('sdc', swiftlm_check.OK,
                      'No errors found on device: sdc'),
                     ('sdd', swiftlm_check.FAIL,
                      'Errors found on device: sdd'))
        for scenario in scenarios:
            expected_dimensions = dict(self.expected_dimensions_base)
            expected_dimensions.update({'device': scenario[0]})
            expected_value_meta = dict(msg=scenario[2])
            expected_metric = dict(self.expected_measurement_base)
            expected_metric.update(dict(dimensions=expected_dimensions,
                                        value=scenario[1],
                                        value_meta=expected_value_meta))
            expected.append(expected_metric)
        self._assert_expected_measurements(expected, actual)

    def test_no_drives_found(self):
        with mock.patch('subprocess.Popen') as mocked:
            mocked.return_value = mock.MagicMock(
                spec=subprocess.Popen,
                communicate=lambda: (None, drive_audit_data.output2))
            with mock.patch('swiftlm.utils.metricdata.timestamp',
                            lambda *args: self.fake_time):
                actual, events, statuses = self.collector.run_checks_d()
        expected_dimensions = dict(self.expected_dimensions_base)
        expected_value_meta = dict(msg='No devices found')
        expected_metric = dict(self.expected_measurement_base)
        expected_metric.update(dict(dimensions=expected_dimensions,
                                    value=swiftlm_check.WARN,
                                    value_meta=expected_value_meta))
        self._assert_expected_measurements([expected_metric], actual)


class TestCheckMounts(BaseTest):
    # rudimentary tests for check_mounts module
    task_name = 'swiftlm.systems.check_mounts'
    task_entry_points = ['check-mounts']
    sub_commands = []

    def test_no_devices_found(self):
        fake_devices_file = os.path.join(self.testdir, 'devices')
        open(fake_devices_file, 'wb')
        with mock.patch(
                'swiftlm.systems.check_mounts.DEVICES', fake_devices_file):
            with mock.patch('swiftlm.utils.metricdata.timestamp',
                            lambda *args: self.fake_time):
                actual, events, statuses = self.collector.run_checks_d()

        expected_dimensions = dict(self.expected_dimensions_base)
        expected_value_meta = dict(msg='No devices found')
        expected_metric = dict(self.expected_measurement_base)
        expected_metric.update(dict(dimensions=expected_dimensions,
                                    value=swiftlm_check.WARN,
                                    value_meta=expected_value_meta))
        self._assert_expected_measurements([expected_metric], actual)

    def test_not_mounted(self):
        fake_devices_file = os.path.join(self.testdir, 'devices')
        scenarios = []
        j = {'devices': []}
        with open(fake_devices_file, 'wb') as f:
            for dev_no, dev in enumerate(('sdd', 'sde')):
                dev_path = os.path.join(self.testdir, 'dev', dev)
                label = 'foo'
                mnt_path = os.path.join(
                    self.testdir, 'mnt', 'disk' + str(dev_no))
                dev_dimensions = {
                    'name': dev_path,
                    'label': label,
                    'swift_drive_name': mnt_path,
                }
                j['devices'].append(dev_dimensions)
                scenarios.append(
                    dict(device=dev_path, label=label, mount=mnt_path))
            json.dump(j, f)
        with mock.patch('swiftlm.systems.check_mounts.DEVICES',
                        fake_devices_file):
            with mock.patch('swiftlm.systems.check_mounts.MOUNT_PATH', ''):
                with mock.patch('swiftlm.utils.metricdata.timestamp',
                                lambda *args: self.fake_time):
                    actual, events, statuses = self.collector.run_checks_d()

        expected = []
        for scenario in scenarios:
            expected_dimensions = dict(self.expected_dimensions_base)
            expected_dimensions.update(scenario)
            expected_value_meta = dict(
                msg='{device} not mounted at {mount}'.format(**scenario))
            expected_metric = dict(self.expected_measurement_base)
            expected_metric.update(dict(dimensions=expected_dimensions,
                                        value=2,
                                        value_meta=expected_value_meta))
            expected.append(expected_metric)
        self._assert_expected_measurements(expected, actual)


def fake_get_ring_hosts(ring_type):
    results = []
    return results


class TestConnectivity(BaseTest):
    # rudimentary test for calling connectivity module
    task_name = 'swiftlm.systems.connectivity'
    task_entry_points = ['connectivity']
    sub_commands = []

    def test_no_drives_found(self):
        @mock.patch('swiftlm.utils.metricdata.timestamp',
                    lambda *args: self.fake_time)
        @mock.patch('swiftlm.systems.connectivity.get_ring_hosts',
                    return_value=[])
        @mock.patch('swiftlm.systems.connectivity.server_type',
                    lambda type: type != ServerType.proxy)
        @mock.patch('swiftlm.systems.connectivity.SWIFT_PATH',
                    self.testdir)
        def do_it(*args):
            actual, events, statuses = self.collector.run_checks_d()
            expected_dimensions = {
                'observer_host': socket.gethostname(),
                'hostname': 'example.com',
                'service': 'object-storage',
            }

            expected_value_meta = dict(msg='No hosts to check')
            expected_metric = dict(name=self.task_name+'.ping_check')
            expected_metric.update(dict(dimensions=expected_dimensions,
                                        value=1,
                                        value_meta=expected_value_meta))
            self._assert_expected_measurements([expected_metric], actual)
        do_it()


def _make_mock_process(stdout, stderr='', returncode=0):
    mock_communicate = mock.MagicMock()
    mock_communicate.return_value = (stdout, stderr)
    mock_process = mock.MagicMock()
    mock_process.communicate = mock_communicate
    mock_process.returncode = returncode
    mock_popen = mock.MagicMock()
    mock_popen.return_value = mock_process
    return mock_popen


class TestCommandLineTasks(BaseTest):
    task_name = 'swiftlm.swiftlm_scan'
    task_entry_points = []

    def test_commands_are_called(self):
        mock_popen = _make_mock_process('')
        self.check.DEFAULT_SUBCOMMANDS = ['drive-audit', 'connectivity']
        with mock.patch(TEST_MODULE + '.subprocess.Popen', mock_popen):
            actual, events, statuses = self.collector.run_checks_d()
        self.assertEqual(2, mock_popen.call_count)
        mock_popen.assert_any_call(
            ['swiftlm-scan', '--format', 'json', '--drive-audit'],
            stdout=mock.ANY, stderr=mock.ANY)
        mock_popen.assert_any_call(
            ['swiftlm-scan', '--format', 'json', '--connectivity'],
            stdout=mock.ANY, stderr=mock.ANY)

    def test_command_returns_metrics_list(self):
        metrics = []
        for v, meta in ((0, 'I am ok'), (1, 'some meta'), (2, 'other meta')):
            metric = dict(metric=self.task_name,
                          timestamp=self.fake_time,
                          dimensions=dict(blah='whatever',
                                          service='object-storage',
                                          hostname=socket.gethostname()),
                          value=v,
                          value_meta=dict(msg=meta))
            metrics.append(metric)
        mock_popen = _make_mock_process(json.dumps(metrics))
        self.check.DEFAULT_SUBCOMMANDS = ['drive-audit']
        self.check.DEFAULT_SUPPRESS_OK = []
        with mock.patch(TEST_MODULE + '.subprocess.Popen', mock_popen):
            actual, events, statuses = self.collector.run_checks_d()

        expected = []
        for metric in metrics:
            expected_dimensions = dict(self.expected_dimensions_base)
            expected_dimensions.update(metric['dimensions'])
            expected_value_meta = metric['value_meta']
            expected_metric = dict(self.expected_measurement_base)
            expected_metric.update(dict(dimensions=expected_dimensions,
                                        value=metric['value'],
                                        value_meta=expected_value_meta))
            expected.append(expected_metric)
        self._assert_expected_measurements(expected, actual)
        mock_popen.assert_called_once_with(
            ['swiftlm-scan', '--format', 'json', '--drive-audit'],
            stdout=mock.ANY, stderr=mock.ANY)

    def test_suppress_ok(self):
        metrics = []
        for v, meta in ((0, 'I am ok'), (1, 'some meta'), (2, 'other meta')):
            metric = dict(metric=self.task_name,
                          timestamp=self.fake_time,
                          dimensions=dict(blah='whatever',
                                          service='object-storage',
                                          hostname=socket.gethostname()),
                          value=v,
                          value_meta=dict(msg=meta))
            metrics.append(metric)
        mock_popen = _make_mock_process(json.dumps(metrics))
        # sanity check - nothing suppressed
        fake_load_instance_config = _make_fake_load_instance_conf(
            subcommands=['drive-audit'])
        with mock.patch(TEST_MODULE + '.SwiftLMScan._load_instance_config',
                        fake_load_instance_config):
            with mock.patch(TEST_MODULE + '.subprocess.Popen', mock_popen):
                actual, events, statuses = self.collector.run_checks_d()

        expected = []
        for metric in metrics:
            expected_dimensions = dict(self.expected_dimensions_base)
            expected_dimensions.update(metric['dimensions'])
            expected_value_meta = metric['value_meta']
            expected_metric = dict(self.expected_measurement_base)
            expected_metric.update(dict(dimensions=expected_dimensions,
                                        value=metric['value'],
                                        value_meta=expected_value_meta))
            expected.append(expected_metric)
        self._assert_expected_measurements(expected, actual)
        mock_popen.assert_called_once_with(
            ['swiftlm-scan', '--format', 'json', '--drive-audit'],
            stdout=mock.ANY, stderr=mock.ANY)

        # set suppress_ok for this task
        fake_load_instance_config = _make_fake_load_instance_conf(
            subcommands=['drive-audit'],
            suppress_ok=['drive-audit'])
        mock_popen.reset_mock()
        with mock.patch(TEST_MODULE + '.SwiftLMScan._load_instance_config',
                        fake_load_instance_config):
            with mock.patch(TEST_MODULE + '.subprocess.Popen', mock_popen):
                actual, events, statuses = self.collector.run_checks_d()

        self._assert_expected_measurements(expected[1:], actual)
        mock_popen.assert_called_once_with(
            ['swiftlm-scan', '--format', 'json', '--drive-audit'],
            stdout=mock.ANY, stderr=mock.ANY)

    def test_command_returns_single_metric(self):
        metric = dict(metric=self.task_name,
                      timestamp=self.fake_time,
                      dimensions=dict(blah='whatever',
                                      service='object-storage',
                                      hostname=socket.gethostname()),
                      value=1,
                      value_meta=dict(msg='some meta'))
        mock_popen = _make_mock_process(json.dumps(metric))
        self.check.DEFAULT_SUBCOMMANDS = ['drive-audit']
        with mock.patch(TEST_MODULE + '.subprocess.Popen', mock_popen):
            actual, events, statuses = self.collector.run_checks_d()

        expected_dimensions = dict(self.expected_dimensions_base)
        expected_dimensions.update(metric['dimensions'])
        expected_value_meta = metric['value_meta']
        expected_metric = dict(self.expected_measurement_base)
        expected_metric.update(dict(dimensions=expected_dimensions,
                                    value=metric['value'],
                                    value_meta=expected_value_meta))
        self._assert_expected_measurements([expected_metric], actual)

    def test_command_returns_junk(self):
        expected_dimensions = dict(self.expected_dimensions_base)
        expected_dimensions.update(dict(
            type="command",
            task="drive-audit"))
        m = 'Command task execution failed: "drive-audit"'
        expected_value_meta = dict(msg=m)
        expected_metric = dict(self.expected_measurement_base)
        expected_metric.update(dict(name=swiftlm_check.MODULE_METRIC_NAME,
                                    dimensions=expected_dimensions,
                                    value=swiftlm_check.FAIL,
                                    value_meta=expected_value_meta))
        expected = [expected_metric]
        mock_popen = _make_mock_process('Hello world!')
        self.check.DEFAULT_SUBCOMMANDS = ['drive-audit']
        with mock.patch(TEST_MODULE + '.subprocess.Popen', mock_popen):
            actual, events, statuses = self.collector.run_checks_d()

        self._assert_expected_measurements(expected, actual)
        log_lines = self.check.log.get_lines_for_level('info')
        self.assertEqual(1, len(log_lines),
                         'Found log lines %s' % log_lines)
        self.assertTrue(log_lines[0].startswith('Ran 1 command task.'),
                        log_lines)
        log_lines = self.check.log.get_lines_for_level('warn')
        self.assertEqual(1, len(log_lines),
                         'Found log lines %s' % log_lines)
        self.assertTrue(log_lines[0].startswith('Failed to parse json:'))

    def test_command_fails(self):
        expected_dimensions = dict(self.expected_dimensions_base)
        expected_dimensions.update(dict(type="command", task="drive-audit"))
        m = 'Command task execution failed: "drive-audit"'
        expected_value_meta = dict(msg=m)
        expected_metric = dict(self.expected_measurement_base)
        expected_metric.update(dict(name=swiftlm_check.MODULE_METRIC_NAME,
                                    dimensions=expected_dimensions,
                                    value=swiftlm_check.FAIL,
                                    value_meta=expected_value_meta))
        expected = [expected_metric]
        mock_popen = _make_mock_process('[{}]', returncode=99)
        self.check.DEFAULT_SUBCOMMANDS = ['drive-audit']
        with mock.patch(TEST_MODULE + '.subprocess.Popen', mock_popen):
            actual, events, statuses = self.collector.run_checks_d()

        self._assert_expected_measurements(expected, actual)
        log_lines = self.check.log.get_lines_for_level('info')
        self.assertEqual(1, len(log_lines),
                         'Found log lines %s' % log_lines)
        self.assertTrue(log_lines[0].startswith('Ran 1 command task.'),
                        log_lines)
        log_lines = self.check.log.get_lines_for_level('warn')
        self.assertEqual(1, len(log_lines),
                         'Found log lines %s' % log_lines)
        self.assertTrue(
            log_lines[0].startswith(
                'Command "swiftlm-scan --format json --drive-audit" '
                'failed with status 99'),
            log_lines[0])

    def test_command_runner_exception(self):
        expected_dimensions = dict(self.expected_dimensions_base)
        expected_dimensions.update(dict(type="command", task="drive-audit"))
        m = 'Command task execution failed: "drive-audit"'
        expected_value_meta = dict(msg=m)
        expected_metric = dict(self.expected_measurement_base)
        expected_metric.update(dict(name=swiftlm_check.MODULE_METRIC_NAME,
                                    dimensions=expected_dimensions,
                                    value=swiftlm_check.FAIL,
                                    value_meta=expected_value_meta))
        expected = [expected_metric]
        self.check.DEFAULT_SUBCOMMANDS = ['drive-audit']
        with mock.patch(
                TEST_MODULE + '.CommandRunner.run_with_timeout') as mock_popen:
            mock_popen.side_effect = Exception('BOOM')
            actual, events, statuses = self.collector.run_checks_d()

        self._assert_expected_measurements(expected, actual)
        log_lines = self.check.log.get_lines_for_level('info')
        self.assertEqual(1, len(log_lines),
                         'Found log lines %s' % log_lines)
        self.assertTrue(log_lines[0].startswith('Ran 1 command task.'),
                        log_lines)
        log_lines = self.check.log.get_lines_for_level('warn')
        self.assertEqual(1, len(log_lines),
                         'Found log lines %s' % log_lines)
        self.assertTrue(
            log_lines[0].startswith(
                'Command "swiftlm-scan --format json --drive-audit" '
                'failed with "BOOM"'),
            log_lines[0])

    def test_command_does_not_exist(self):
        expected_dimensions = dict(self.expected_dimensions_base)
        command = "this_test_may_break_if_this_command_did_exist"
        expected_dimensions.update(dict(type='command',
                                        task="lala"))
        m = 'Command task execution failed: "lala"'
        expected_value_meta = dict(msg=m)
        expected_metric = dict(self.expected_measurement_base)
        expected_metric.update(dict(name=swiftlm_check.MODULE_METRIC_NAME,
                                    dimensions=expected_dimensions,
                                    value=swiftlm_check.FAIL,
                                    value_meta=expected_value_meta))
        expected = [expected_metric]
        self.check.DEFAULT_SUBCOMMANDS = ['lala']
        self.check.COMMAND_ARGS = [command]
        actual, events, statuses = self.collector.run_checks_d()

        self._assert_expected_measurements(expected, actual)
        log_lines = self.check.log.get_lines_for_level('info')
        self.assertEqual(1, len(log_lines),
                         'Found log lines %s' % log_lines)
        self.assertTrue(log_lines[0].startswith('Ran 1 command task.'),
                        log_lines)
        log_lines = self.check.log.get_lines_for_level('warn')
        self.assertEqual(1, len(log_lines),
                         'Found log lines %s' % log_lines)
        self.assertTrue(
            log_lines[0].startswith(
                'Command "this_test_may_break_if_this_command_did_exist '
                '--lala" failed with "[Errno 2]'),
            log_lines[0])

    def test_command_timeout(self):
        expected_dimensions = dict(self.expected_dimensions_base)
        expected_dimensions.update(dict(type='command', task="/bin/sleep 0.1"))
        m = 'Command task execution timed out: "/bin/sleep 0.1"'
        expected_value_meta = dict(msg=m)
        expected_metric = dict(self.expected_measurement_base)
        expected_metric.update(dict(name=swiftlm_check.MODULE_METRIC_NAME,
                                    dimensions=expected_dimensions,
                                    value=swiftlm_check.FAIL,
                                    value_meta=expected_value_meta))
        expected = [expected_metric]
        self.check.DEFAULT_SUBCOMMANDS = ['0.1']
        self.check.SUBCOMMAND_PREFIX = ''
        self.check.COMMAND_ARGS = ['/bin/sleep']
        self.check.COMMAND_TIMEOUT = 0.01
        actual, events, statuses = self.collector.run_checks_d()

        self._assert_expected_measurements(expected, actual)
        log_lines = self.check.log.get_lines_for_level('info')
        self.assertEqual(1, len(log_lines),
                         'Found log lines %s' % log_lines)
        self.assertTrue(log_lines[0].startswith('Ran 1 command task.'),
                        log_lines)
        log_lines = self.check.log.get_lines_for_level('warn')
        self.assertEqual(1, len(log_lines),
                         'Found log lines %s' % log_lines)
        self.assertTrue(
            log_lines[0].startswith(
                'Command "/bin/sleep 0.1" timed out after 0.01s'))


class TestLoadFileTasks(BaseTest):
    task_name = 'swiftlm.swiftlm_scan'
    task_entry_points = []
    sub_commands = []

    def test_run_load_file_task(self):
        metrics = []
        for v, meta in ((0, 'I am ok'), (1, 'some meta'), (2, 'other meta')):
            metric = dict(metric=self.task_name,
                          timestamp=self.fake_time,
                          dimensions=dict(blah='whatever',
                                          service='object-storage'),
                          value=v,
                          value_meta=dict(msg=meta))
            metrics.append(metric)

        # read ok
        metric_file = os.path.join(self.testdir, 'afile.json')
        with open(metric_file, 'wb') as f:
            json.dump(metrics, f)
        # actual = self.check._run_load_file_task(metric_file)
        # self.assertEqual(metrics, actual)

        # can't read a locked file
        with utility.lock_file(metric_file, unlink=False):
            with mock.patch(TEST_MODULE + '.time.sleep') as mock_sleep:
                actual = self.check._run_load_file_task(metric_file)
        expected_dimensions = dict(self.expected_dimensions_base)
        expected_dimensions.update(dict(type='file', task=metric_file))
        m = 'File task execution failed: "%s"' % metric_file
        expected_value_meta = dict(msg=m)
        expected_metric = dict(metric=swiftlm_check.MODULE_METRIC_NAME,
                               dimensions=expected_dimensions,
                               value=swiftlm_check.FAIL,
                               value_meta=expected_value_meta)
        self.assertEqual(expected_metric, actual)
        self.assertEqual(5, mock_sleep.call_count)

        # ...until it is unlocked
        with utility.lock_file(metric_file, unlink=False):
            with mock.patch(TEST_MODULE + '.time.sleep') as mock_sleep:
                with mock.patch(TEST_MODULE + '.fcntl.flock') as mock_flock:
                    err = IOError()
                    err.errno = errno.EWOULDBLOCK
                    # fake lock becoming available on 3rd attempt
                    mock_flock.side_effect = [err, err, None]
                    actual = self.check._run_load_file_task(metric_file)
        self.assertEqual(metrics, actual)
        # 2 sleeps for 2 IOErrors
        self.assertEqual(2, mock_sleep.call_count)
        self.assertEqual(3, mock_flock.call_count)

    def test_metrics_list_loaded(self):
        metrics = []
        for v, meta in ((0, 'I am ok'), (1, 'some meta'), (2, 'other meta')):
            metric = dict(metric=self.task_name,
                          timestamp=self.fake_time,
                          dimensions=dict(blah='whatever',
                                          service='object-storage'),
                          value=v,
                          value_meta=dict(msg=meta))
            metrics.append(metric)

        metric_file = os.path.join(self.testdir, 'afile.json')
        with open(metric_file, 'wb') as f:
            json.dump(metrics, f)

        fake_load_instance_config = _make_fake_load_instance_conf(
            [metric_file])
        with mock.patch(TEST_MODULE + '.SwiftLMScan._load_instance_config',
                        fake_load_instance_config):
            actual, events, statuses = self.collector.run_checks_d()

        expected = []
        for metric in metrics:
            expected_dimensions = dict(self.expected_dimensions_base)
            expected_dimensions.update(metric['dimensions'])
            # reported metric dimensions do not include hostname so agent
            # will add its configured hostname to dimensions...
            expected_dimensions['hostname'] = 'example.com'
            expected_value_meta = metric['value_meta']
            expected_metric = dict(self.expected_measurement_base)
            expected_metric.update(dict(dimensions=expected_dimensions,
                                        value=metric['value'],
                                        value_meta=expected_value_meta))
            expected.append(expected_metric)
        self._assert_expected_measurements(expected, actual)

    def test_file_not_found(self):
        metric_file = os.path.join(self.testdir, 'afile.json')

        fake_load_instance_config = _make_fake_load_instance_conf(
            [metric_file])
        with mock.patch(TEST_MODULE + '.SwiftLMScan._load_instance_config',
                        fake_load_instance_config):
            actual, events, statuses = self.collector.run_checks_d()

        expected_dimensions = dict(self.expected_dimensions_base)
        expected_dimensions.update(dict(type='file', task=metric_file))
        m = 'File task execution failed: "%s"' % metric_file
        expected_value_meta = dict(msg=m)
        expected_metric = dict(self.expected_measurement_base)
        expected_metric.update(dict(name=swiftlm_check.MODULE_METRIC_NAME,
                                    dimensions=expected_dimensions,
                                    value=swiftlm_check.FAIL,
                                    value_meta=expected_value_meta))
        expected = [expected_metric]
        self._assert_expected_measurements(expected, actual)

    def test_file_not_json(self):
        metric_file = os.path.join(self.testdir, 'afile.json')
        with open(metric_file, 'wb') as f:
            f.write('this is not json')

        fake_load_instance_config = _make_fake_load_instance_conf(
            [metric_file])
        with mock.patch(TEST_MODULE + '.SwiftLMScan._load_instance_config',
                        fake_load_instance_config):
            actual, events, statuses = self.collector.run_checks_d()

        expected_dimensions = dict(self.expected_dimensions_base)
        expected_dimensions.update(dict(type='file', task=metric_file))
        m = 'File task execution failed: "%s"' % metric_file
        expected_value_meta = dict(msg=m)
        expected_metric = dict(self.expected_measurement_base)
        expected_metric.update(dict(name=swiftlm_check.MODULE_METRIC_NAME,
                                    dimensions=expected_dimensions,
                                    value=swiftlm_check.FAIL,
                                    value_meta=expected_value_meta))
        expected = [expected_metric]
        self._assert_expected_measurements(expected, actual)
